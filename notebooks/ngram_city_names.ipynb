{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d5e2524",
   "metadata": {},
   "source": [
    "This notebook aims to teach:\n",
    "\n",
    "How to build a simple autoregressive generative model that produces names resembling city names from around the world\n",
    "\n",
    "How increasing context size (bigram → trigram → 4-gram) leads to lower model loss and improved predictions\n",
    "\n",
    "How a bigram model can be implemented using a traditional, rule-based software approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064dbe15",
   "metadata": {},
   "source": [
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146d9935",
   "metadata": {},
   "source": [
    "1. Lets create a generative model to generate new city names\n",
    "\n",
    "- Cities dataset consists of existing city names around the world - lets take a peek at the data"
   ]
  },
  {
   "cell_type": "code",
   "id": "5f4233db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T16:05:00.368529Z",
     "start_time": "2026-01-28T16:05:00.343959Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"../dataset/cities_latin_alphabet.csv\")\n",
    "df.head()\n",
    "\n",
    "# . indicates end of a city name - this will be useful later when we are generating new city names and want to know when to stop\n",
    "df[\"city\"] = df[\"city\"] + '.'"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "77912866",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T16:05:02.177302Z",
     "start_time": "2026-01-28T16:05:02.162487Z"
    }
   },
   "source": [
    "city_counts_by_country = df.groupby(\"country\")[\"city\"].count().sort_values(ascending=False).head(3)\n",
    "\n",
    "print(f'We have {len(df)} city names from {len(set(df[\"country\"]))} countries')\n",
    "\n",
    "print('City Names by Country:')\n",
    "print(city_counts_by_country)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 35320 city names from 185 countries\n",
      "City Names by Country:\n",
      "country\n",
      "Russia           3768\n",
      "Philippines      3161\n",
      "United States    2929\n",
      "Name: city, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "730c9fa7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T16:05:12.325038Z",
     "start_time": "2026-01-28T16:05:12.309035Z"
    }
   },
   "source": [
    "import string\n",
    "\n",
    "# vocab should be all characters from df[city] lowercase\n",
    "all_cities = df[\"city\"].astype(str).str.lower()\n",
    "vocab = sorted(set(\"\".join(all_cities)))\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print(f'we have a vocabulary of {vocab_size} characters')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have a vocabulary of 27 characters\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "295fdf87",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T16:05:18.353582Z",
     "start_time": "2026-01-28T16:05:18.338639Z"
    }
   },
   "source": [
    "vocab"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Lets define some utility functions to encode/decode city names to/from integer sequences\n",
    "where encode maps characters to integers and decode maps integers back to characters"
   ],
   "id": "6656bd551b9ab8a4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T16:05:22.514213Z",
     "start_time": "2026-01-28T16:05:22.504372Z"
    }
   },
   "cell_type": "code",
   "source": "df.head()",
   "id": "594bc7384c6091a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "        city               country\n",
       "0    Encamp.               Andorra\n",
       "1   Canillo.               Andorra\n",
       "2   Sharjah.  United Arab Emirates\n",
       "3     Dubai.  United Arab Emirates\n",
       "4  Asadabad.           Afghanistan"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Encamp.</td>\n",
       "      <td>Andorra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Canillo.</td>\n",
       "      <td>Andorra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sharjah.</td>\n",
       "      <td>United Arab Emirates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dubai.</td>\n",
       "      <td>United Arab Emirates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Asadabad.</td>\n",
       "      <td>Afghanistan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "03ded0f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T16:05:41.734657Z",
     "start_time": "2026-01-28T16:05:41.726355Z"
    }
   },
   "source": [
    "stoi = {char: idx for idx, char in enumerate(vocab)}\n",
    "itos = {idx: char for char, idx in stoi.items()}\n",
    "\n",
    "def encode(s):\n",
    "    return [stoi[c] for c in s]\n",
    "def decode(ids):\n",
    "    return ''.join([itos[i] for i in ids])\n"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "example encode/decode",
   "id": "2f5cbf4cefe872b8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T16:05:44.065027Z",
     "start_time": "2026-01-28T16:05:44.061880Z"
    }
   },
   "cell_type": "code",
   "source": [
    "moscow_ids = encode('moscow')\n",
    "print(f'encoded: {moscow_ids}')\n",
    "print(f'decoded: {decode(moscow_ids)}')"
   ],
   "id": "c288ce99b1deb200",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded: [13, 15, 19, 3, 15, 23]\n",
      "decoded: moscow\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Lets define a simple bigram model using a multi-layer perceptron (MLP)\n",
    "\n",
    "ℹ️ if the details of the model are unclear, dont worry - we will cover them in more detail on other notebooks"
   ],
   "id": "b7197788876de705"
  },
  {
   "cell_type": "code",
   "id": "8a87a071",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T16:05:57.317736Z",
     "start_time": "2026-01-28T16:05:55.978428Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class BigramMLP(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, 128)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, vocab_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)   # (B, 128)\n",
    "        return self.mlp(x) # (B, V)"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "b9215b3e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T16:05:58.417635Z",
     "start_time": "2026-01-28T16:05:58.410223Z"
    }
   },
   "source": [
    "model= BigramMLP(vocab_size)\n",
    "print(model)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BigramMLP(\n",
      "  (embed): Embedding(27, 128)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=27, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "lets define a function to generate city names using the the model\n",
   "id": "6d42d5bead6304d6"
  },
  {
   "cell_type": "code",
   "id": "d77bf864",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T16:06:00.364735Z",
     "start_time": "2026-01-28T16:06:00.360915Z"
    }
   },
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def generate(model, start_char, max_len=20):\n",
    "    idx = torch.tensor([stoi[start_char]])\n",
    "    out = start_char\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        logits = model(idx)\n",
    "        probs = F.softmax(logits[-1], dim=-1)\n",
    "        next_idx = torch.multinomial(probs, 1).item()\n",
    "        next_char = itos[next_idx]\n",
    "        out += next_char\n",
    "        if next_char == '.':\n",
    "            break\n",
    "        idx = torch.tensor([next_idx])\n",
    "\n",
    "    return out\n"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "try and generate a city name  using untrained model",
   "id": "96271d18b9105d94"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T16:06:50.083440Z",
     "start_time": "2026-01-28T16:06:50.065340Z"
    }
   },
   "cell_type": "code",
   "source": "generate(BigramMLP(vocab_size), 's')",
   "id": "7e8e7c935d2b3cb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sfmyqszbmbipvqvlwoblc'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Doesnt sound like a city name, lets see if we can train the model to generate better city names\n",
    "\n",
    "First we need to prepare the training data for bigram model"
   ],
   "id": "ea8827ba04970343"
  },
  {
   "cell_type": "code",
   "id": "76892890",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T16:06:16.440357Z",
     "start_time": "2026-01-28T16:06:16.273567Z"
    }
   },
   "source": [
    "X, Y = [], []\n",
    "\n",
    "for name in df[\"city\"]:\n",
    "    name = name.lower()\n",
    "    for a, b in zip(name[:-1], name[1:]):\n",
    "\n",
    "        X.append(stoi[a])\n",
    "        Y.append(stoi[b])\n",
    "\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "id": "cd3141c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T16:06:19.389427Z",
     "start_time": "2026-01-28T16:06:18.585960Z"
    }
   },
   "source": [
    "model = BigramMLP(vocab_size)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-3)\n"
   ],
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "id": "ac78dfb1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T16:06:23.970821Z",
     "start_time": "2026-01-28T16:06:19.758153Z"
    }
   },
   "source": [
    "batch_size = 64\n",
    "steps = 4000\n",
    "\n",
    "for step in range(steps):\n",
    "    idx = torch.randint(0, len(X), (batch_size,))\n",
    "    xb = X[idx]\n",
    "    yb = Y[idx]\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    logits = model(xb)\n",
    "    loss = loss_fn(logits, yb)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if step % 500 == 0:\n",
    "        print(f\"step {step} | loss {loss.item():.4f}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 | loss 3.2882\n",
      "step 500 | loss 2.8779\n",
      "step 1000 | loss 2.6168\n",
      "step 1500 | loss 2.5690\n",
      "step 2000 | loss 2.5155\n",
      "step 2500 | loss 2.7143\n",
      "step 3000 | loss 2.4291\n",
      "step 3500 | loss 2.5338\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "lets generate some city names using the trained model",
   "id": "a71c892230f941f2"
  },
  {
   "cell_type": "code",
   "id": "d90fbd53",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T16:07:08.699563Z",
     "start_time": "2026-01-28T16:07:08.694644Z"
    }
   },
   "source": "generate(model, 's')",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'stenez.'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Sounds slightly more like a city name after training - but can we do better?\n",
    "\n",
    "- So far we have only used the previous character to predict the next character (bigram model) - what if we used the previous two characters (trigram model)?"
   ],
   "id": "82a6a9dceff33c2f"
  },
  {
   "cell_type": "markdown",
   "id": "0aa6a3f0",
   "metadata": {},
   "source": [
    "# Trigram"
   ]
  },
  {
   "cell_type": "code",
   "id": "773fa5f5",
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TrigramMLP(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, 128)\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(128 * 2, 256),  # two embeddings concatenated\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, vocab_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, 2)\n",
    "        emb = self.embed(x)          # (B, 2, 128)\n",
    "        emb = emb.view(x.size(0), -1)  # (B, 256)\n",
    "        return self.mlp(emb)         # (B, V)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "51c2f50a",
   "metadata": {},
   "source": [
    "X, Y = [], []\n",
    "\n",
    "for name in names:\n",
    "    name = name.lower()\n",
    "    for a, b, c in zip(name[:-2], name[1:-1], name[2:]):\n",
    "        X.append([stoi[a], stoi[b]])\n",
    "        Y.append(stoi[c])\n",
    "\n",
    "X = torch.tensor(X)  # (N, 2)\n",
    "Y = torch.tensor(Y)  # (N,)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "96d5fe83",
   "metadata": {},
   "source": [
    "model = TrigramMLP(vocab_size)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-3)\n",
    "\n",
    "batch_size = 256\n",
    "steps = 4000\n",
    "\n",
    "for step in range(steps):\n",
    "    idx = torch.randint(0, len(X), (batch_size,))\n",
    "    xb = X[idx]   # (B, 2)\n",
    "    yb = Y[idx]   # (B,)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    logits = model(xb)\n",
    "    loss = loss_fn(logits, yb)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if step % 500 == 0:\n",
    "        print(f\"step {step} | loss {loss.item():.4f}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "notice that the loss is lower than the bigram model - this is because the model has more context to make better predictions",
   "id": "ca17aa56d7cc96b1"
  },
  {
   "cell_type": "code",
   "id": "c704fe92",
   "metadata": {},
   "source": [
    "def generate(model, start_chars, max_len=20):\n",
    "    assert len(start_chars) == 2\n",
    "\n",
    "    idx = [stoi[start_chars[0]], stoi[start_chars[1]]]\n",
    "    out = start_chars\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        x = torch.tensor([idx])  # (1, 2)\n",
    "        logits = model(x)\n",
    "        probs = F.softmax(logits[0], dim=-1)\n",
    "\n",
    "        next_idx = torch.multinomial(probs, 1).item()\n",
    "        next_char = itos[next_idx]\n",
    "\n",
    "        out += next_char\n",
    "        if next_char == '.':\n",
    "            break\n",
    "\n",
    "        idx = [idx[1], next_idx]  # slide window\n",
    "\n",
    "    return out\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5d25f7df",
   "metadata": {},
   "source": [
    "generate(model, 'ca')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Not bad! The generated names sound a bit better, althought they are still not perfect\n",
    "\n",
    "- Lets try increasing the context size even further to 4-grams"
   ],
   "id": "1016277f5103e5dc"
  },
  {
   "cell_type": "markdown",
   "id": "3fac7014",
   "metadata": {},
   "source": [
    "# 4-gram"
   ]
  },
  {
   "cell_type": "code",
   "id": "a0f9263f",
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FourGramMLP(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=128):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim * 4, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, vocab_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, 4)\n",
    "        emb = self.embed(x)            # (B, 4, 128)\n",
    "        emb = emb.view(x.size(0), -1)  # (B, 512)\n",
    "        return self.mlp(emb)           # (B, V)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5ae725d3",
   "metadata": {},
   "source": [
    "X, Y = [], []\n",
    "\n",
    "for name in names:\n",
    "    name = name.lower()\n",
    "    if len(name) < 5:\n",
    "        continue\n",
    "\n",
    "    for a, b, c, d, e in zip(\n",
    "        name[:-4], name[1:-3], name[2:-2], name[3:-1], name[4:]\n",
    "    ):\n",
    "        X.append([stoi[a], stoi[b], stoi[c], stoi[d]])\n",
    "        Y.append(stoi[e])\n",
    "\n",
    "X = torch.tensor(X)  # (N, 4)\n",
    "Y = torch.tensor(Y)  # (N,)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4ee5b1d1",
   "metadata": {},
   "source": [
    "model = FourGramMLP(vocab_size)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-3)\n",
    "\n",
    "batch_size = 256\n",
    "steps = 4000\n",
    "\n",
    "for step in range(steps):\n",
    "    idx = torch.randint(0, len(X), (batch_size,))\n",
    "    xb = X[idx]   # (B, 4)\n",
    "    yb = Y[idx]   # (B,)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    logits = model(xb)\n",
    "    loss = loss_fn(logits, yb)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if step % 500 == 0:\n",
    "        print(f\"step {step} | loss {loss.item():.4f}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "looks like the loss has decreased even further - the model is able to make better predictions with more context",
   "id": "a689becc21354db4"
  },
  {
   "cell_type": "code",
   "id": "c4fe3978",
   "metadata": {},
   "source": [
    "def generate(model, start_text, max_len=50):\n",
    "    assert len(start_text) >= 4\n",
    "\n",
    "    context = [stoi[c] for c in start_text[-4:]]\n",
    "    out = start_text\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        x = torch.tensor([context])  # (1, 4)\n",
    "        logits = model(x)\n",
    "        probs = F.softmax(logits[0], dim=-1)\n",
    "        print(probs)\n",
    "\n",
    "        next_idx = torch.multinomial(probs, 1).item()\n",
    "        next_char = itos[next_idx]\n",
    "\n",
    "        out += next_char\n",
    "        if next_char == '.':\n",
    "            break\n",
    "\n",
    "        context = context[1:] + [next_idx]\n",
    "\n",
    "    return out\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "98fec8e6",
   "metadata": {},
   "source": [
    "generate(model, 'casa')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "ok it seems like this is the best we can do with this simple MLP",
   "id": "15c60f7dc2de862c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now lets try an build a bigram model using a more traditional, rule-based software approach",
   "id": "48e9a84df029a9fa"
  },
  {
   "cell_type": "code",
   "id": "d0dd5b07",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T16:09:16.335456Z",
     "start_time": "2026-01-28T16:09:16.100356Z"
    }
   },
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Count character bigrams from city names\n",
    "bigram_counts = Counter()\n",
    "\n",
    "for name in df[\"city\"]:\n",
    "    name_lower = name.lower()\n",
    "    # Create bigrams by pairing consecutive characters\n",
    "    for i in range(len(name_lower) - 1):\n",
    "        bigram = name_lower[i:i+2]\n",
    "        bigram_counts[bigram] += 1\n",
    "\n",
    "# Display most common bigrams\n",
    "print(f\"Total unique bigrams: {len(bigram_counts)}\")\n",
    "print(f\"\\nTop 20 most common character bigrams:\")\n",
    "for bigram, count in bigram_counts.most_common(20):\n",
    "    print(f\"  '{bigram}': {count}\")\n",
    "\n",
    "# Convert to dictionary for easy access\n",
    "bigram_dict = dict(bigram_counts)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique bigrams: 654\n",
      "\n",
      "Top 20 most common character bigrams:\n",
      "  'a.': 7293\n",
      "  'an': 6403\n",
      "  'ar': 4161\n",
      "  'n.': 4073\n",
      "  'al': 3514\n",
      "  'ra': 3446\n",
      "  'in': 3317\n",
      "  'e.': 3302\n",
      "  'i.': 3167\n",
      "  'la': 3113\n",
      "  'en': 2994\n",
      "  'o.': 2985\n",
      "  'er': 2835\n",
      "  'ma': 2778\n",
      "  'ng': 2666\n",
      "  'on': 2560\n",
      "  'ta': 2514\n",
      "  'na': 2496\n",
      "  'ha': 2351\n",
      "  'ba': 2326\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "id": "42f04b12",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T16:09:42.856759Z",
     "start_time": "2026-01-28T16:09:42.830957Z"
    }
   },
   "source": [
    "import random\n",
    "\n",
    "def generate_from_bigram_dict(start_char, max_len=20):\n",
    "    \"\"\"\n",
    "    Generate text using bigram dictionary counts.\n",
    "    Selects next character based on bigram frequency.\n",
    "    \"\"\"\n",
    "    if start_char not in vocab:\n",
    "        return f\"Error: '{start_char}' not in vocabulary\"\n",
    "    \n",
    "    output = start_char\n",
    "    current_char = start_char.lower()\n",
    "    \n",
    "    for _ in range(max_len):\n",
    "        # Find all bigrams that start with current_char\n",
    "        possible_bigrams = [(bigram, count) for bigram, count in bigram_dict.items() \n",
    "                           if bigram[0] == current_char]\n",
    "        \n",
    "        if not possible_bigrams:\n",
    "            # No bigrams found starting with this character, stop\n",
    "            break\n",
    "        \n",
    "        # Extract next characters and their counts\n",
    "        next_chars = []\n",
    "        weights = []\n",
    "        for bigram, count in possible_bigrams:\n",
    "            next_char = bigram[1]\n",
    "            next_chars.append(next_char)\n",
    "            weights.append(count)\n",
    "        \n",
    "        # Weighted random selection based on bigram counts\n",
    "        next_char = random.choices(next_chars, weights=weights, k=1)[0]\n",
    "        output += next_char\n",
    "        \n",
    "        # Stop if we hit the end marker\n",
    "        if next_char == '.':\n",
    "            break\n",
    "        \n",
    "        current_char = next_char\n",
    "    \n",
    "    return output\n",
    "\n",
    "# Test generation\n",
    "print(\"Generating city names using bigram dictionary:\")\n",
    "for start in ['m', 's', 'k', 'n', 'v']:\n",
    "    generated = generate_from_bigram_dict(start)\n",
    "    print(f\"  '{start}' -> '{generated}'\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating city names using bigram dictionary:\n",
      "  'm' -> 'maire.'\n",
      "  's' -> 'selsad.'\n",
      "  'k' -> 'koviserntestina.'\n",
      "  'n' -> 'nirbalo.'\n",
      "  'v' -> 'vimaski.'\n"
     ]
    }
   ],
   "execution_count": 35
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
