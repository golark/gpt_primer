{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d5e2524",
   "metadata": {},
   "source": [
    "This notebook aims to teach:\n",
    "\n",
    "How to build a simple autoregressive generative model that produces names resembling city names from around the world\n",
    "\n",
    "How increasing context size (bigram → trigram → 4-gram) leads to lower model loss and improved predictions\n",
    "\n",
    "How a bigram model can be implemented using a traditional, rule-based software approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064dbe15",
   "metadata": {},
   "source": [
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146d9935",
   "metadata": {},
   "source": [
    "1. Lets create a generative model to generate new city names\n",
    "\n",
    "- Cities dataset consists of existing city names around the world - lets take a peek at the data"
   ]
  },
  {
   "cell_type": "code",
   "id": "5f4233db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-25T20:46:52.335893Z",
     "start_time": "2026-01-25T20:46:51.824836Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"../dataset/cities_latin_alphabet.csv\")\n",
    "df.head()\n",
    "\n",
    "# . indicates end of a city name - this will be useful later when we are generating new city names and want to know when to stop\n",
    "df[\"city\"] = df[\"city\"] + '.'"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "77912866",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-25T20:46:53.876211Z",
     "start_time": "2026-01-25T20:46:53.863993Z"
    }
   },
   "source": [
    "city_counts_by_country = df.groupby(\"country\")[\"city\"].count().sort_values(ascending=False).head(3)\n",
    "\n",
    "print(f'We have {len(df)} city names from {len(set(df[\"country\"]))} countries')\n",
    "\n",
    "print('City Names by Country:')\n",
    "print(city_counts_by_country)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 35320 city names from 185 countries\n",
      "City Names by Country:\n",
      "country\n",
      "Russia           3768\n",
      "Philippines      3161\n",
      "United States    2929\n",
      "Name: city, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "730c9fa7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-25T20:46:55.770048Z",
     "start_time": "2026-01-25T20:46:55.756841Z"
    }
   },
   "source": [
    "import string\n",
    "\n",
    "# vocab should be all characters from df[city] lowercase\n",
    "all_cities = df[\"city\"].astype(str).str.lower()\n",
    "vocab = sorted(set(\"\".join(all_cities)))\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print(f'we have a vocabulary of {vocab_size} characters')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have a vocabulary of 27 characters\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "295fdf87",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-25T20:46:57.239959Z",
     "start_time": "2026-01-25T20:46:57.233677Z"
    }
   },
   "source": [
    "vocab"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Lets define some utility functions to encode/decode city names to/from integer sequences\n",
    "where encode maps characters to integers and decode maps integers back to characters"
   ],
   "id": "6656bd551b9ab8a4"
  },
  {
   "cell_type": "code",
   "id": "03ded0f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-25T20:46:59.003197Z",
     "start_time": "2026-01-25T20:46:58.991816Z"
    }
   },
   "source": [
    "stoi = {char: idx for idx, char in enumerate(vocab)}\n",
    "itos = {idx: char for char, idx in stoi.items()}\n",
    "\n",
    "def encode(s):\n",
    "    return [stoi[c] for c in s]\n",
    "def decode(ids):\n",
    "    return ''.join([itos[i] for i in ids])\n"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "example encode/decode",
   "id": "2f5cbf4cefe872b8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-25T20:47:00.883631Z",
     "start_time": "2026-01-25T20:47:00.877951Z"
    }
   },
   "cell_type": "code",
   "source": [
    "moscow_ids = encode('moscow')\n",
    "print(f'encoded: {moscow_ids}')\n",
    "print(f'decoded: {decode(moscow_ids)}')"
   ],
   "id": "c288ce99b1deb200",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded: [13, 15, 19, 3, 15, 23]\n",
      "decoded: moscow\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Lets define a simple bigram model using a multi-layer perceptron (MLP)\n",
    "\n",
    "ℹ️ if the details of the model are unclear, dont worry - we will cover them in more detail on other notebooks"
   ],
   "id": "b7197788876de705"
  },
  {
   "cell_type": "code",
   "id": "8a87a071",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-25T20:47:03.645644Z",
     "start_time": "2026-01-25T20:47:02.353369Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class BigramMLP(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, 128)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, vocab_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)   # (B, 128)\n",
    "        return self.mlp(x) # (B, V)"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "b9215b3e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-25T20:47:04.880339Z",
     "start_time": "2026-01-25T20:47:04.867246Z"
    }
   },
   "source": [
    "model= BigramMLP(vocab_size)\n",
    "print(model)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BigramMLP(\n",
      "  (embed): Embedding(27, 128)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=27, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "lets define a function to generate city names using the the model\n",
   "id": "6d42d5bead6304d6"
  },
  {
   "cell_type": "code",
   "id": "d77bf864",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-25T20:47:06.739268Z",
     "start_time": "2026-01-25T20:47:06.733895Z"
    }
   },
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def generate(model, start_char, max_len=20):\n",
    "    idx = torch.tensor([stoi[start_char]])\n",
    "    out = start_char\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        logits = model(idx)\n",
    "        probs = F.softmax(logits[-1], dim=-1)\n",
    "        next_idx = torch.multinomial(probs, 1).item()\n",
    "        next_char = itos[next_idx]\n",
    "        out += next_char\n",
    "        if next_char == '.':\n",
    "            break\n",
    "        idx = torch.tensor([next_idx])\n",
    "\n",
    "    return out\n"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "try and generate a city name  using untrained model",
   "id": "96271d18b9105d94"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-25T20:47:11.170525Z",
     "start_time": "2026-01-25T20:47:11.137047Z"
    }
   },
   "cell_type": "code",
   "source": "generate(BigramMLP(vocab_size), 'a')",
   "id": "7e8e7c935d2b3cb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abhwejohedkujfqsutust'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Doesnt sound like a city name, lets see if we can train the model to generate better city names\n",
    "\n",
    "First we need to prepare the training data for bigram model"
   ],
   "id": "ea8827ba04970343"
  },
  {
   "cell_type": "code",
   "id": "76892890",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-25T20:48:06.149145Z",
     "start_time": "2026-01-25T20:48:05.937287Z"
    }
   },
   "source": [
    "X, Y = [], []\n",
    "\n",
    "for name in df[\"city\"]:\n",
    "    name = name.lower()\n",
    "    for a, b in zip(name[:-1], name[1:]):\n",
    "\n",
    "        X.append(stoi[a])\n",
    "        Y.append(stoi[b])\n",
    "\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "id": "cd3141c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-25T20:50:23.664563Z",
     "start_time": "2026-01-25T20:50:23.645679Z"
    }
   },
   "source": [
    "model = BigramMLP(vocab_size)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-3)\n"
   ],
   "outputs": [],
   "execution_count": 69
  },
  {
   "cell_type": "code",
   "id": "ac78dfb1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-25T20:50:29.593853Z",
     "start_time": "2026-01-25T20:50:24.946842Z"
    }
   },
   "source": [
    "batch_size = 64\n",
    "steps = 4000\n",
    "\n",
    "for step in range(steps):\n",
    "    idx = torch.randint(0, len(X), (batch_size,))\n",
    "    xb = X[idx]\n",
    "    yb = Y[idx]\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    logits = model(xb)\n",
    "    loss = loss_fn(logits, yb)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if step % 500 == 0:\n",
    "        print(f\"step {step} | loss {loss.item():.4f}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 | loss 3.3063\n",
      "step 500 | loss 2.6583\n",
      "step 1000 | loss 2.5657\n",
      "step 1500 | loss 2.6863\n",
      "step 2000 | loss 2.3661\n",
      "step 2500 | loss 2.6160\n",
      "step 3000 | loss 2.5860\n",
      "step 3500 | loss 2.6361\n"
     ]
    }
   ],
   "execution_count": 70
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "lets generate some city names using the trained model",
   "id": "a71c892230f941f2"
  },
  {
   "cell_type": "code",
   "id": "d90fbd53",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-25T20:50:30.949050Z",
     "start_time": "2026-01-25T20:50:30.943633Z"
    }
   },
   "source": "generate(model, 'a')",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'amangoka.'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 71
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Sounds slightly more like a city name after training - but can we do better?\n",
    "\n",
    "- So far we have only used the previous character to predict the next character (bigram model) - what if we used the previous two characters (trigram model)?"
   ],
   "id": "82a6a9dceff33c2f"
  },
  {
   "cell_type": "markdown",
   "id": "0aa6a3f0",
   "metadata": {},
   "source": [
    "# Trigram"
   ]
  },
  {
   "cell_type": "code",
   "id": "773fa5f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-25T20:51:35.530918Z",
     "start_time": "2026-01-25T20:51:35.519925Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TrigramMLP(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, 128)\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(128 * 2, 256),  # two embeddings concatenated\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, vocab_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, 2)\n",
    "        emb = self.embed(x)          # (B, 2, 128)\n",
    "        emb = emb.view(x.size(0), -1)  # (B, 256)\n",
    "        return self.mlp(emb)         # (B, V)\n"
   ],
   "outputs": [],
   "execution_count": 72
  },
  {
   "cell_type": "code",
   "id": "51c2f50a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-25T20:51:37.582521Z",
     "start_time": "2026-01-25T20:51:37.532163Z"
    }
   },
   "source": [
    "X, Y = [], []\n",
    "\n",
    "for name in names:\n",
    "    name = name.lower()\n",
    "    for a, b, c in zip(name[:-2], name[1:-1], name[2:]):\n",
    "        X.append([stoi[a], stoi[b]])\n",
    "        Y.append(stoi[c])\n",
    "\n",
    "X = torch.tensor(X)  # (N, 2)\n",
    "Y = torch.tensor(Y)  # (N,)\n"
   ],
   "outputs": [],
   "execution_count": 73
  },
  {
   "cell_type": "code",
   "id": "96d5fe83",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-25T20:51:49.206804Z",
     "start_time": "2026-01-25T20:51:41.567619Z"
    }
   },
   "source": [
    "model = TrigramMLP(vocab_size)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-3)\n",
    "\n",
    "batch_size = 256\n",
    "steps = 4000\n",
    "\n",
    "for step in range(steps):\n",
    "    idx = torch.randint(0, len(X), (batch_size,))\n",
    "    xb = X[idx]   # (B, 2)\n",
    "    yb = Y[idx]   # (B,)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    logits = model(xb)\n",
    "    loss = loss_fn(logits, yb)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if step % 500 == 0:\n",
    "        print(f\"step {step} | loss {loss.item():.4f}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 | loss 3.2997\n",
      "step 500 | loss 1.7136\n",
      "step 1000 | loss 1.6755\n",
      "step 1500 | loss 1.5399\n",
      "step 2000 | loss 1.6804\n",
      "step 2500 | loss 1.5885\n",
      "step 3000 | loss 1.4636\n",
      "step 3500 | loss 1.6712\n"
     ]
    }
   ],
   "execution_count": 74
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "notice that the loss is lower than the bigram model - this is because the model has more context to make better predictions",
   "id": "ca17aa56d7cc96b1"
  },
  {
   "cell_type": "code",
   "id": "c704fe92",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-25T20:52:02.746101Z",
     "start_time": "2026-01-25T20:52:02.736341Z"
    }
   },
   "source": [
    "def generate(model, start_chars, max_len=20):\n",
    "    assert len(start_chars) == 2\n",
    "\n",
    "    idx = [stoi[start_chars[0]], stoi[start_chars[1]]]\n",
    "    out = start_chars\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        x = torch.tensor([idx])  # (1, 2)\n",
    "        logits = model(x)\n",
    "        probs = F.softmax(logits[0], dim=-1)\n",
    "\n",
    "        next_idx = torch.multinomial(probs, 1).item()\n",
    "        next_char = itos[next_idx]\n",
    "\n",
    "        out += next_char\n",
    "        if next_char == '.':\n",
    "            break\n",
    "\n",
    "        idx = [idx[1], next_idx]  # slide window\n",
    "\n",
    "    return out\n"
   ],
   "outputs": [],
   "execution_count": 75
  },
  {
   "cell_type": "code",
   "id": "5d25f7df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-25T20:52:48.222214Z",
     "start_time": "2026-01-25T20:52:48.217256Z"
    }
   },
   "source": [
    "generate(model, 'ca')"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cayski.'"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 130
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Not bad! The generated names sound a bit better, althought they are still not perfect\n",
    "\n",
    "- Lets try increasing the context size even further to 4-grams"
   ],
   "id": "1016277f5103e5dc"
  },
  {
   "cell_type": "markdown",
   "id": "3fac7014",
   "metadata": {},
   "source": [
    "# 4-gram"
   ]
  },
  {
   "cell_type": "code",
   "id": "a0f9263f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-25T20:53:58.483638Z",
     "start_time": "2026-01-25T20:53:58.469620Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FourGramMLP(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=128):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim * 4, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, vocab_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, 4)\n",
    "        emb = self.embed(x)            # (B, 4, 128)\n",
    "        emb = emb.view(x.size(0), -1)  # (B, 512)\n",
    "        return self.mlp(emb)           # (B, V)\n"
   ],
   "outputs": [],
   "execution_count": 131
  },
  {
   "cell_type": "code",
   "id": "5ae725d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-25T20:54:00.889008Z",
     "start_time": "2026-01-25T20:54:00.834225Z"
    }
   },
   "source": [
    "X, Y = [], []\n",
    "\n",
    "for name in names:\n",
    "    name = name.lower()\n",
    "    if len(name) < 5:\n",
    "        continue\n",
    "\n",
    "    for a, b, c, d, e in zip(\n",
    "        name[:-4], name[1:-3], name[2:-2], name[3:-1], name[4:]\n",
    "    ):\n",
    "        X.append([stoi[a], stoi[b], stoi[c], stoi[d]])\n",
    "        Y.append(stoi[e])\n",
    "\n",
    "X = torch.tensor(X)  # (N, 4)\n",
    "Y = torch.tensor(Y)  # (N,)\n"
   ],
   "outputs": [],
   "execution_count": 132
  },
  {
   "cell_type": "code",
   "id": "4ee5b1d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-25T20:54:11.800619Z",
     "start_time": "2026-01-25T20:54:02.633916Z"
    }
   },
   "source": [
    "model = FourGramMLP(vocab_size)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-3)\n",
    "\n",
    "batch_size = 256\n",
    "steps = 4000\n",
    "\n",
    "for step in range(steps):\n",
    "    idx = torch.randint(0, len(X), (batch_size,))\n",
    "    xb = X[idx]   # (B, 4)\n",
    "    yb = Y[idx]   # (B,)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    logits = model(xb)\n",
    "    loss = loss_fn(logits, yb)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if step % 500 == 0:\n",
    "        print(f\"step {step} | loss {loss.item():.4f}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 | loss 3.2915\n",
      "step 500 | loss 0.9166\n",
      "step 1000 | loss 0.8324\n",
      "step 1500 | loss 0.6900\n",
      "step 2000 | loss 0.7456\n",
      "step 2500 | loss 0.6131\n",
      "step 3000 | loss 0.6533\n",
      "step 3500 | loss 0.5539\n"
     ]
    }
   ],
   "execution_count": 133
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "looks like the loss has decreased even further - the model is able to make better predictions with more context",
   "id": "a689becc21354db4"
  },
  {
   "cell_type": "code",
   "id": "c4fe3978",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-25T20:54:23.136561Z",
     "start_time": "2026-01-25T20:54:23.129687Z"
    }
   },
   "source": [
    "def generate(model, start_text, max_len=50):\n",
    "    assert len(start_text) >= 4\n",
    "\n",
    "    context = [stoi[c] for c in start_text[-4:]]\n",
    "    out = start_text\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        x = torch.tensor([context])  # (1, 4)\n",
    "        logits = model(x)\n",
    "        probs = F.softmax(logits[0], dim=-1)\n",
    "\n",
    "        next_idx = torch.multinomial(probs, 1).item()\n",
    "        next_char = itos[next_idx]\n",
    "\n",
    "        out += next_char\n",
    "        if next_char == '.':\n",
    "            break\n",
    "\n",
    "        context = context[1:] + [next_idx]\n",
    "\n",
    "    return out\n"
   ],
   "outputs": [],
   "execution_count": 134
  },
  {
   "cell_type": "code",
   "id": "98fec8e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-25T20:54:24.960383Z",
     "start_time": "2026-01-25T20:54:24.952452Z"
    }
   },
   "source": [
    "generate(model, 'casa')"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'casalimki.'"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 135
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "ok it seems like this is the best we can do with this simple MLP",
   "id": "15c60f7dc2de862c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now lets try an build a bigram model using a more traditional, rule-based software approach",
   "id": "48e9a84df029a9fa"
  },
  {
   "cell_type": "code",
   "id": "d0dd5b07",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-25T20:55:25.702079Z",
     "start_time": "2026-01-25T20:55:25.677277Z"
    }
   },
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Count character bigrams from city names\n",
    "bigram_counts = Counter()\n",
    "\n",
    "for name in names:\n",
    "    name_lower = name.lower()\n",
    "    # Create bigrams by pairing consecutive characters\n",
    "    for i in range(len(name_lower) - 1):\n",
    "        bigram = name_lower[i:i+2]\n",
    "        bigram_counts[bigram] += 1\n",
    "\n",
    "# Display most common bigrams\n",
    "print(f\"Total unique bigrams: {len(bigram_counts)}\")\n",
    "print(f\"\\nTop 20 most common character bigrams:\")\n",
    "for bigram, count in bigram_counts.most_common(20):\n",
    "    print(f\"  '{bigram}': {count}\")\n",
    "\n",
    "# Convert to dictionary for easy access\n",
    "bigram_dict = dict(bigram_counts)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique bigrams: 401\n",
      "\n",
      "Top 20 most common character bigrams:\n",
      "  '..': 3768\n",
      "  'sk': 1184\n",
      "  'ov': 894\n",
      "  'a.': 888\n",
      "  'ka': 702\n",
      "  'y.': 678\n",
      "  'no': 630\n",
      "  'vo': 622\n",
      "  'ko': 564\n",
      "  'o.': 559\n",
      "  'ya': 555\n",
      "  'ki': 529\n",
      "  'ye': 499\n",
      "  'in': 492\n",
      "  'k.': 466\n",
      "  'ay': 391\n",
      "  'oy': 387\n",
      "  'iy': 381\n",
      "  'ch': 377\n",
      "  'sh': 374\n"
     ]
    }
   ],
   "execution_count": 136
  },
  {
   "cell_type": "code",
   "id": "42f04b12",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-25T20:55:34.200571Z",
     "start_time": "2026-01-25T20:55:34.193277Z"
    }
   },
   "source": [
    "import random\n",
    "\n",
    "def generate_from_bigram_dict(start_char, max_len=20):\n",
    "    \"\"\"\n",
    "    Generate text using bigram dictionary counts.\n",
    "    Selects next character based on bigram frequency.\n",
    "    \"\"\"\n",
    "    if start_char not in vocab:\n",
    "        return f\"Error: '{start_char}' not in vocabulary\"\n",
    "    \n",
    "    output = start_char\n",
    "    current_char = start_char.lower()\n",
    "    \n",
    "    for _ in range(max_len):\n",
    "        # Find all bigrams that start with current_char\n",
    "        possible_bigrams = [(bigram, count) for bigram, count in bigram_dict.items() \n",
    "                           if bigram[0] == current_char]\n",
    "        \n",
    "        if not possible_bigrams:\n",
    "            # No bigrams found starting with this character, stop\n",
    "            break\n",
    "        \n",
    "        # Extract next characters and their counts\n",
    "        next_chars = []\n",
    "        weights = []\n",
    "        for bigram, count in possible_bigrams:\n",
    "            next_char = bigram[1]\n",
    "            next_chars.append(next_char)\n",
    "            weights.append(count)\n",
    "        \n",
    "        # Weighted random selection based on bigram counts\n",
    "        next_char = random.choices(next_chars, weights=weights, k=1)[0]\n",
    "        output += next_char\n",
    "        \n",
    "        # Stop if we hit the end marker\n",
    "        if next_char == '.':\n",
    "            break\n",
    "        \n",
    "        current_char = next_char\n",
    "    \n",
    "    return output\n",
    "\n",
    "# Test generation\n",
    "print(\"Generating city names using bigram dictionary:\")\n",
    "for start in ['m', 's', 'k', 'n', 'v']:\n",
    "    generated = generate_from_bigram_dict(start)\n",
    "    print(f\"  '{start}' -> '{generated}'\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating city names using bigram dictionary:\n",
      "  'm' -> 'mayalzno.'\n",
      "  's' -> 'ska.'\n",
      "  'k' -> 'kirgosvlkadank.'\n",
      "  'n' -> 'nyskrmutolodadzmba.'\n",
      "  'v' -> 'vovrtns.'\n"
     ]
    }
   ],
   "execution_count": 137
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
